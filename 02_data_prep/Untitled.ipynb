{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify python install location above /home/ec2-user/SageMaker so that we don't have to \n",
    "#reinstall custom packages after every reboot.\n",
    "\n",
    "import subprocess, sys, os, site\n",
    "\n",
    "user_libs_path=os.path.expanduser(\"~\")+\"/SageMaker/.local\"\n",
    "\n",
    "if not os.path.exists(user_libs_path):\n",
    "    os.makedirs(user_libs_path)\n",
    "\n",
    "sys.path.insert(0, user_libs_path+'/lib/python3.6/site-packages')\n",
    "site.USER_BASE=user_libs_path\n",
    "\n",
    "my_env = os.environ.copy()\n",
    "my_env[\"PYTHONUSERBASE\"] = user_libs_path\n",
    "\n",
    "process = subprocess.run(\"pip install -U --quiet sagemaker_pyspark\"\n",
    "                           .split(), env=my_env, stdout=subprocess.PIPE)\n",
    "process.stderr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/SageMaker/.local/lib/python3.6/site-packages',\n",
       " '~/SageMaker/.local/lib/python3.6/site-packages',\n",
       " '~/SageMaker/.local/lib/python3.6/site-packages',\n",
       " '/tmp/spark-eeedd1bc-418e-40fe-a616-f4e8df631026/userFiles-4c5701b9-6ac2-41f1-bd51-e65f6ec0bdc3',\n",
       " '',\n",
       " '/home/ec2-user/anaconda3/envs/python3/lib/python36.zip',\n",
       " '/home/ec2-user/anaconda3/envs/python3/lib/python3.6',\n",
       " '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/lib-dynload',\n",
       " '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages',\n",
       " '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/ec2-user/.ipython']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"Anonymize PySpark\").getOrCreate()\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    args = dict(zip(args_iter, args_iter))\n",
    "    \n",
    "    #sample args for interactive testing\n",
    "    #args = {'project_bucket': 'project1-lz', 'input_table': 'upload', 'output_table': 'raw', 'database': 'default', 'file_name': 'Tweets.csv'}\n",
    "\n",
    "    project_bucket = args['project_bucket']\n",
    "    input_table = args['input_table']\n",
    "    output_table = args['output_table']\n",
    "    database = args['database']\n",
    "    input_s3_uri = 's3://' + project_bucket + '/' + input_table + '/' + args['file_name']\n",
    "    output_s3_uri = 's3://' + project_bucket + '/' + output_table + '/' + args['file_name'].split('.')[0] + '-anon/'\n",
    "\n",
    "    # Interactive pyspark from glue development endpoint allows reaading from glue crawlers\n",
    "    # from awsglue.context import GlueContext\n",
    "    #glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "    # Create a dataframe from glue catalog\n",
    "    #df = glueContext.create_data_frame.from_catalog(database=database, table_name=input_table)\n",
    "\n",
    "    #Print out information about this data\n",
    "    #print(\"Count:  \", df.count())\n",
    "    #df.printSchema()\n",
    "\n",
    "    df = spark.read.csv(input_s3_uri, header=True)\n",
    "\n",
    "    # replace each tweeters name with crc bigint\n",
    "    dfAnnocrc = df.withColumn(\"annonym\", sha2(\"name\", 256)).select(\"annonym\", \"tweet_id\", \"airline\", \"airline_sentiment\", \"text\")\n",
    "\n",
    "    # write back to s3 as parquet\n",
    "    dfAnnocrc.write.mode(\"append\").parquet(output_s3_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table': {'Name': 'upload',\n",
       "  'DatabaseName': 'default',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2020, 11, 9, 15, 22, 5, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2020, 11, 9, 15, 22, 5, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2020, 11, 9, 15, 22, 5, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'tweet_id', 'Type': 'bigint'},\n",
       "    {'Name': 'airline_sentiment', 'Type': 'string'},\n",
       "    {'Name': 'airline_sentiment_confidence', 'Type': 'double'},\n",
       "    {'Name': 'negativereason', 'Type': 'string'},\n",
       "    {'Name': 'negativereason_confidence', 'Type': 'double'},\n",
       "    {'Name': 'airline', 'Type': 'string'},\n",
       "    {'Name': 'airline_sentiment_gold', 'Type': 'string'},\n",
       "    {'Name': 'name', 'Type': 'string'},\n",
       "    {'Name': 'negativereason_gold', 'Type': 'string'},\n",
       "    {'Name': 'retweet_count', 'Type': 'bigint'},\n",
       "    {'Name': 'text', 'Type': 'string'},\n",
       "    {'Name': 'tweet_coord', 'Type': 'array<double>'},\n",
       "    {'Name': 'tweet_created', 'Type': 'string'},\n",
       "    {'Name': 'tweet_location', 'Type': 'string'},\n",
       "    {'Name': 'user_timezone', 'Type': 'string'}],\n",
       "   'Location': 's3://project1-lz/upload/',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'UPDATED_BY_CRAWLER': 'project1',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'averageRecordSize': '190',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'commentCharacter': '#',\n",
       "    'compressionType': 'none',\n",
       "    'delimiter': ',',\n",
       "    'objectCount': '1',\n",
       "    'recordCount': '18007',\n",
       "    'sizeKey': '3421431',\n",
       "    'skip.header.line.count': '1',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'UPDATED_BY_CRAWLER': 'project1',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'averageRecordSize': '190',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'commentCharacter': '#',\n",
       "   'compressionType': 'none',\n",
       "   'delimiter': ',',\n",
       "   'objectCount': '1',\n",
       "   'recordCount': '18007',\n",
       "   'sizeKey': '3421431',\n",
       "   'skip.header.line.count': '1',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::000378343852:assumed-role/AWSGlueServiceRole-crawler-raw/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '000378343852'},\n",
       " 'ResponseMetadata': {'RequestId': '19db5190-a496-4e83-a454-a6b11c31e611',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sat, 14 Nov 2020 15:53:40 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2280',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '19db5190-a496-4e83-a454-a6b11c31e611'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "gl_client = boto3.client('glue')\n",
    "response = gl_client.get_table(DatabaseName='default', Name='upload')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./aws-glue-libs\")\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.context import GlueContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"hive.metastore.schema.verification\", \"false\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://ip-172-31-43-209.eu-west-1.compute.internal:8020/user/spark/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '36025'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.host', 'ip-172-16-10-201.eu-west-1.compute.internal'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sts-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-aws-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-kms-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-common-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemaker-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-auth-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/sagemaker-spark_2.11-spark_2.2.0-1.4.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-annotations-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemakerruntime-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-s3-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-core-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/htrace-core4-4.0.1-incubating.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('hive.metastore.client.factory.class',\n",
       "  'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1605364984390'),\n",
       " ('hive.metastore.schema.verification', 'false'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
